{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Prove that $D_{f^*}(x^*, y^*) = D_f(y,x)$ and $x^*=\\nabla f(x), x^*=\\nabla f(y)$, $D_h$ is a Bergman divergence w.r.t. to function $h$. We assume that $f$ is closed and $\\nabla f^* = (\\nabla f)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $f^*(y)$ exists, then exists $x^*$ for which $f^*(y)=y^Tx^*-f(x^*)$. $x^*$ maximizes function $g(x)=y^Tx-f(x)$ so $\\nabla g(x^*) = y - \\nabla f(x^*)=0$ and we get $y=\\nabla f(x^*)$\n",
    "\n",
    "Using the result from above we get $f^*(\\nabla f(x)) = x^T \\nabla f(x) - f(x)=f^*(x^*)$, $f^*(y^*) = y^T \\nabla f(y) - f(y))$.\n",
    "\n",
    "Now let's substitute this into $D_{f^*}(x^*, y^*)$ $$D_{f^*}(x^*, y^*) = x^T \\nabla f(x)-f(x)-y^T\\nabla f(y) _f(y)-(\\nabla f^*(\\nabla f(y)), \\nabla f(x) - \\nabla f(y)) = x^T \\nabla f(x)-f(x)-y^T\\nabla f(y) _f(y)-(y, \\nabla f(x) - \\nabla f(y)) = f(y)-f(x)+x^T\\nabla f(x) - y^T\\nabla f(y) - y^T \\nabla f(x) + y^T\\nabla f(y) = f(y)-f(x)+x^T \\nabla f(x) - y^T\\nabla f(x) = f(y-f(x) - (\\nabla f(x), y-x) = D(y, x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "**A projection on the probability simplex.** Find Eucledian projection on the probability simplex, $\\Sigma_{i=1}^nx_i = 1$, $\\forall i x_i \\geq 0$. Could you formulate it as an optimization problem and write its dual? Could you provide an analytic form expression for this problem (if any)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{minimize}}\n",
    "& & \\frac{1}{2}||x-y||_2^2 \\\\\n",
    "& \\text{subject to}\n",
    "& & x_i \\geq 0, \\; i = 1, \\ldots, n \\\\\n",
    "& & & \\mathbf{1}^T x = 1\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(x, \\lambda, \\mu) = \\frac{1}{2}||x-y||_2^2 + \\lambda^T x + \\mu (\\mathbf{1}^Tx-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_x L = x - y + \\lambda + \\mu \\mathbf{1} = 0$ from which we get $x^* = y - \\lambda - \\mu \\mathbf{1}$ and so $g(\\lambda, \\mu) = \\frac{1}{2}||\\lambda + \\mu \\mathbf{1}||_2^2 + (\\lambda + \\mu \\mathbf{1})^T (y - \\lambda - \\mu \\mathbf{1}) - \\mu = -\\frac{1}{2}||\\lambda + \\mu \\mathbf{1}||_2^2 + (\\lambda + \\mu \\mathbf{1})^T y - \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the dual is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\lambda, \\mu}{\\text{maximize}}\n",
    "& & -\\frac{1}{2}||\\lambda + \\mu \\mathbf{1}||_2^2 + (\\lambda + \\mu \\mathbf{1})^T y - \\mu \\\\\n",
    "& \\text{subject to}\n",
    "& & \\lambda_i \\geq 0, \\; i = 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the simple and determenistic algorithm how to find projection exists and and can be found in https://arxiv.org/pdf/1309.1541.pdf\n",
    "\n",
    "I'll consider a little simplier in terms of obvioucity and a little less efficient in terms of computation cost.\n",
    "\n",
    "$x = max(y + \\lambda \\mathbf{1}, 0)$, where $\\lambda$ is chosen to set $\\Sigma x_i = 1$\n",
    "\n",
    "The equation above can be proven to be correct from geometric point of view - normal to hyperplane $\\Sigma x_i = 1$ is $\\mathbf{1}/\\sqrt{n}$ so a projection to that hyperplane will be $x=y+\\lambda \\mathbf{1}$ for some $\\lambda$, but probability simplex is restriction of that hyperplane to $\\mathbf{R}_+^n$ so some components (actually several smallest components, exploiting the normal equality in each direction again) will be zeros and we should just find $\\lambda$ that will set $\\Sigma x_i = 1$\n",
    "\n",
    "The proper $\\lambda$ may be found by trying to set number of non-zero components $\\rho$ from 1 to $n$, when $\\lambda_{\\rho} = \\frac{1}{\\rho}(1 - \\sum\\limits_{i=1}^{\\rho}u_i)$, where $u$ is sorted in descended order $y_i$. When just comparing $||x-y||$ choose the nearest one. The proposed algo is quite obvious and do not need long proofs. For more formal description and a little faster algo adress the paper. I do not wish to rewrite everything from where. For what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust non-parametric classification.** We consider a problem of non-parametric classification: given a set of points $\\{x_i,y_i\\}^n_{i=1}, x_i \\in R^d, y_i \\in \\{Â±1\\}$, one needs to find a maximal margin linear separator of the positively and negatively labeled sets of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Formulate the problem as an optimization one, propose a convex relaxation of this problem.\n",
    "\n",
    "(b) Relate the points as centers of ellipsoids $\\bar{x_i} = \\{x\\text{ | }||\\Sigma_i(x - x_i)||_2 \\leq 1\\}$. Formulate the problem and relax it to convex one.\n",
    "\n",
    "(c) Set $\\Sigma_i = \\mu_i I$, $\\mu_i \\in U[0,1]$. Positive class is sampled from $N(20* \\mathbf{1}, 20* \\mathbf{1})$, negative class is sampled from $N(40 * \\mathbf{1}, 15 * \\mathbf{1}$. Solve (b) with this sample for (n, d) = (100, 20), (1000, 200), (10000, 200)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& & & \\underset{\\omega, b}{\\text{max }} \\underset{x_i}{\\text{min }} |(x_i, \\frac{\\omega}{|\\omega|}) + b| \\\\\n",
    "& \\text{subject to}\n",
    "& & sign((x_i, \\omega) + b) = sign(y_i)\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity let's drop bias (include it to omega increasing the dimension of $x$ by 1 and setting $x_i^{d+1}=1$)\n",
    "\n",
    "The constraint can be written as $(x_i,\\omega)y_i > 0$. Now using the fact that for any $x$ we can find $w$ to force $(x, w) = 1$, supposing $x$ is the closest $x_i$ we now know the distance to the closest point is exactly $\\frac{1}{|\\omega|}$. As we forced the closest point to have inner product 1, all of the rest points will have inner product at least 1, so we can rewrite constraint as $(x_i, \\omega)y_i \\geq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& & & \\underset{\\omega}{\\text{max }}\\frac{1}{|\\omega|} \\\\\n",
    "& \\text{subject to}\n",
    "& & (x_i, \\omega) \\geq 1\\text{ for i }\\in 1, 2, \\dots, n\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& & & \\underset{\\omega}{\\text{min }}|\\omega| \\\\\n",
    "& \\text{subject to}\n",
    "& & (x_i, \\omega) \\geq 1\\text{ for i }\\in 1, 2, \\dots, n\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& & & \\underset{\\omega}{\\text{min }}|\\omega| \\\\\n",
    "& \\text{subject to}\n",
    "& & \\underset{||\\Sigma_i \\Delta x_i||_2 \\leq 1}{\\text{min }}y_i((\\omega, x+\\Delta x) + b) \\geq 1\\text{ for i }\\in 1, 2, \\dots, n\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{||\\Sigma_i \\Delta x_i||_2 \\leq 1}{\\text{min }}y_i((\\omega, \\Delta x)) = -||(\\Sigma_i^T)^{-1} \\omega||_2$ which can be recieved from dual norms (p=2 is self-dual)\n",
    "\n",
    "so\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& & & \\underset{\\omega}{\\text{min }}|\\omega| \\\\\n",
    "& \\text{subject to}\n",
    "& & y_i((\\omega, x) + b) - ||(\\Sigma_i^T)^{-1} \\omega||_2 \\geq 1\\text{ for i }\\in 1, 2, \\dots, n\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
